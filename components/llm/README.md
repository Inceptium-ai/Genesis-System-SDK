# LLM Components

This directory contains components for integrating Large Language Models (LLMs) into applications.

## ğŸ“ Structure

```
llm/
â”œâ”€â”€ langchain/
â”‚   â””â”€â”€ structured-output/  # LangChain with_structured_output (framework approach)
â”‚
â””â”€â”€ api-direct/
    â”œâ”€â”€ fastapi-ai-service/ # Complete FastAPI + LLM service template
    â””â”€â”€ structured-output/  # 100% RELIABLE structured output (no LangChain)
```

## ğŸ¯ When to Use Which

```
â”Œâ”€ Do you need structured/typed JSON output?
â”‚   â”‚
â”‚   â”œâ”€â”€ YES
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ Want minimal dependencies? â†’ api-direct/structured-output
â”‚   â”‚   â”‚   - 100% RELIABLE (guaranteed valid output or clear exception)
â”‚   â”‚   â”‚   - Uses httpx + Pydantic + json-repair
â”‚   â”‚   â”‚   - No LangChain dependency
â”‚   â”‚   â”‚   - Perfect for apps like Resumax
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ Already using LangChain? â†’ langchain/structured-output
â”‚   â”‚       - Uses LangChain with_structured_output()
â”‚   â”‚       - Integrates with LangChain ecosystem
â”‚   â”‚       - Provider-agnostic (OpenAI, Anthropic, etc.)
â”‚   â”‚
â”‚   â””â”€â”€ NO (simple prompt â†’ text response)
â”‚       â””â”€â”€ api-direct/fastapi-ai-service
â”‚           - Complete FastAPI + LLM template
â”‚           - Basic OpenRouter integration
â”‚           - Good starting point for AI backends
â”‚
â””â”€ Do you need agentic/multi-step workflows?
    â””â”€â”€ YES â†’ Consider LangGraph (future component)
```

## ğŸ“Š Comparison

| Feature | api-direct/structured-output | langchain/structured-output | api-direct/fastapi-ai-service |
|---------|------------------------------|----------------------------|------------------------------|
| **Reliability** | 100% guaranteed | ~98% | Manual handling |
| **Dependencies** | httpx, pydantic, json-repair | langchain-core | httpx |
| **LangChain** | âŒ Not required | âœ… Required | âŒ Not required |
| **JSON Repair** | âœ… Built-in | âŒ No | âŒ No |
| **Retries** | âœ… Auto w/ backoff | âœ… Built-in | âŒ Manual |
| **Best For** | Production apps | LangChain projects | Simple backends |

## ğŸ”Œ Provider Support

### Via LangChain
- OpenAI (GPT-4, GPT-3.5)
- Anthropic (Claude 3.5/3)
- Google (Gemini)
- Local models (Ollama)

### Via OpenRouter (api-direct)
- All OpenAI models
- All Anthropic models
- Many open-source models
- Single API key for all

## âš ï¸ Common Issues

### JSON Parsing Failures

**Problem:** LLM returns malformed JSON
**Solution:** Use `langchain/structured-output` which:
1. First tries native provider structured output
2. Falls back to function/tool calling
3. Falls back to prompt + Pydantic parser
4. Auto-retries on validation failure

### Rate Limits

**Problem:** 429 errors from provider
**Solution:** Both components include:
- Exponential backoff
- Retry logic
- Rate limit headers handling

### Token Limits

**Problem:** Response truncated
**Solution:** 
- Use streaming for long outputs
- Chunk large documents before processing
- Set explicit max_tokens in requests

## ğŸ“– Examples

### Example: Resume Analysis (Structured Output)

```python
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_openai import ChatOpenAI

class ResumeAnalysis(BaseModel):
    score: int = Field(description="Overall score 0-100")
    strengths: list[str] = Field(description="Key strengths")
    weaknesses: list[str] = Field(description="Areas to improve")

llm = ChatOpenAI(model="gpt-4o")
structured_llm = llm.with_structured_output(ResumeAnalysis)

result = structured_llm.invoke("Analyze this resume: ...")
# result is GUARANTEED to be valid ResumeAnalysis
```

### Example: Simple Chat (Direct API)

```python
import httpx

async with httpx.AsyncClient() as client:
    response = await client.post(
        "https://openrouter.ai/api/v1/chat/completions",
        headers={"Authorization": f"Bearer {api_key}"},
        json={
            "model": "anthropic/claude-3-haiku",
            "messages": [{"role": "user", "content": "Hello!"}]
        }
    )
    print(response.json()["choices"][0]["message"]["content"])
```

## ğŸ†• Adding New LLM Components

When adding new LLM components:

1. Place in appropriate subdirectory (`langchain/` or `api-direct/`)
2. Include `component.yaml` with:
   - Required dependencies
   - Environment variables needed
   - Common gotchas
3. Include working code examples
4. Document provider compatibility
5. Add error handling patterns

---

*Last updated: December 2025*
